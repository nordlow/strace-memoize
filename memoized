#!/usr/bin/env python3

# TODO Fix crash when building ~/Work/dmd

# TODO Cache stdout and stderr

# Get current time before call to strace and the modtime for each input after
# and use to more safely remove written files in opened (potential input)
# directories. Reuse watchman here to faster discard directories that have are
# touched but whose content haven't changed.

# TODO Reuse `watchman`:
# - Home: https://facebook.github.io/watchman/
# - Repo: https://github.com/facebook/watchman

# TODO Lookup command and list of inputs from output artifact

# TODO activate checking of `access` inputs

# TODO warn about certain output paths?
# TODO warn when inputs and outputs

# TODO save and load
# - non-zero exist codes in calls to sys.exit()
# - stdout

# TODO how to handle stat mtime on input dirs where output files are written?

# TODO Update modification times in cache if they are same as written file

# TODO Add `env` argument to `memoized_run` and use it instead of `os.environ._data` if set

# TODO Detect file system race conditions in child processes

# TODO ask on StackExchange if there's a flag to strace to not show failing
# system calls

# TODO Ask Claes Wallin how to fingerprint the Linux kernel version or distribution
# - cat /proc/version
# - cat /proc/mounts

# TODO use gcc -pipe

# TODO add git origin (repo:commit) to input files and sort them on git repo origin
# GIT_REPO:GIT_COMMIT
# TAB rel_path_file_a
# TAB rel_path_file_b
# TAB ...

# TODO parse 'st_mtime using Python's datetime.strptime(some_string, '')

# TODO directory chash is chash of list of file names in directory

# TODO use ltrace and reduce env deps by catching all getenv

# TODO add server that does inotify watches for all dirs containing input and
# output files

# TODO Add automatic distribution of execution via chroot. System libs and programs can be hard-linked out read-only from cache

import sys
import os
import subprocess
import os.path
import re
import hashlib
import tempfile
import shutil
import gzip
import pickle                   # cPickle not available in Python3
import binascii
import getopt

from pprint import pprint as pp # convenient shorthand

# from dbgio import dln

# show results
OPT_VERBOSE = True # False, True, 'raw'

SYS_CALLS = ['execve', 'open', 'openat', 'stat', 'stat64', 'statfs', 'chdir'# , 'access'
]
SYS_CALLS_AT = ['openat']       # Linux

# content hashing algorithm
HASH_NAME = 'sha256'

INDENT_STEP = 2                 # number of spaces per indentation level

include_system_deps = True

SYSTEM_FILE_RE = re.compile(r'/+(usr|lib|lib64|dev|proc|etc)')
SKIP_FILE_RE = re.compile(r'/+(tmp|dev/urandom|dev/pts|dev/null)')

ENCODING = 'utf-8'              # default text encoding

CACHE_DIR = '~/.cache'          # cache directory
MEMOIZE_DIR = os.path.join(os.path.expanduser(CACHE_DIR), 'memoize')
ARTIFACT_DIR = os.path.join(MEMOIZE_DIR, 'artifacts', HASH_NAME.upper())

INPUTS_FILENAME = 'inputs'
OUTPUTS_FILENAME = 'outputs'
INPUTS_FILE_CONTENT_HASH_PREFIX = INPUTS_FILENAME + '_' + HASH_NAME.upper() + '-'

MAX_STRACE_STRING_LENGTH = 4096

SEPARATOR = b'	'               # tab

SKIP_SYSTEM_INPUTS = False


def dir_content_is_modified_since(fname, mtime, chash):
    if os.path.getmtime(fname) != mtime: # robust to NFS time skews
        dir_hash = hashlib.new(HASH_NAME)
        dir_filenames_hash(fname, dir_hash) # TODO needed only for stat?
        return dir_hash.digest() != chash
    else:
        return False


def regular_file_content_is_modified_since(fname, mtime, chash):
    if os.path.getmtime(fname) != mtime: # robust to NFS time skews
        return hashlib.new(HASH_NAME, open(fname, 'rb').read()).digest() != chash
    else:
        return False


def file_content_is_modified_since(fname, mtime, chash):
    if os.path.isdir(fname):
        return dir_content_is_modified_since(fname, mtime, chash)
    else:
        return regular_file_content_is_modified_since(fname, mtime, chash)


def _is_content_hashable_file(fname):
    """Check if FNAME is a file whose contents can be read and hashed."""
    return not fname.startswith(b'/dev/') and os.path.exists(fname)


def _indent_space(indent):
    return ' '*INDENT_STEP*indent


def _print_syscall_fnames(syscall_s, fnames, indent):
    if not fnames:
        return

    if OPT_VERBOSE == 'raw':
        print(syscall_s + ':')
        for fname in sorted(fnames):
            print(fname)
    else:
        print('syscall=' + syscall_s + '():')

        prev_dname = None

        for fname in filter(lambda fname: (include_system_deps or
                                           not (re.match(pattern=SYSTEM_FILE_RE,
                                                         string=bytes.decode(fname)) and
                                                re.match(pattern=SKIP_FILE_RE,
                                                         string=bytes.decode(fname)))),
                            sorted(fnames, reverse=True)):

            dname = os.path.dirname(fname)
            dname_qs = '"' + bytes.decode(dname) + '"'
            bname_qs = '"' + bytes.decode(os.path.basename(fname)) + '"'

            if dname != prev_dname:
                print(_indent_space(indent) + '@ ' + dname_qs + ':')

            indent += 1
            if _is_content_hashable_file(fname):
                mtime = os.path.getmtime(fname)
                if syscall_s in ('stat', 'statfs', 'access') or os.path.isdir(fname):
                    print(_indent_space(indent) + '- ' + bname_qs + ': mtime=' + str(mtime))
                else:
                    chash = hashlib.new(HASH_NAME, open(fname, 'rb').read())
                    chash_s = ' ' + HASH_NAME + "=" + chash.hexdigest()
                    print(_indent_space(indent) + '- ' + bname_qs + ': mtime=' + str(mtime) + chash_s)
            indent -= 1

            prev_dname = dname


def chash_update_tree(chash, curr):
    if (isinstance(curr, bytes)):
        chash.update(curr)
    elif (isinstance(curr, str)):
        chash.update(curr.encode(ENCODING))
    elif (isinstance(curr, list) or
          isinstance(curr, tuple)):
        chash.update(repr(len(curr)).encode(ENCODING))
        for sub in curr:
            chash_update_tree(chash, sub)
    elif (isinstance(curr, dict)):
        chash.update(repr(len(curr)).encode(ENCODING))
        for sub_key, sub_value in sorted(curr).items(): # requires sortedness
            chash_update_tree(chash, sub_key)
            chash_update_tree(chash, sub_value)
    elif (isinstance(curr, float)):
        chash.update(str(curr).encode(ENCODING))
    elif (isinstance(curr, filter)):
        for sub in curr:
            chash_update_tree(chash, sub)
    elif type(curr) is type(hashlib.new(HASH_NAME)): # TODO use _hashlib.HASH instead
        chash.update(curr.hexdigest().encode(ENCODING))
    else:
        raise Exception("Handle curr type" + str(type(curr)))


def chash_exec_state(exec_state):
    """Create fingerprint (content hash) of call to system function `execve()`
described by parameter EXEC_STATE."""

    chash = hashlib.new(HASH_NAME)

    execve_fname = exec_state[0][1]
    chash.update(open(execve_fname, 'rb').read()) # contents of executed binary

    chash.update(os.getcwd().encode(ENCODING))     # current working directory. TODO needed?

    chash_update_tree(chash, exec_state)         # the rest

    return chash


def cache_dir(exec_hexdigest):
    return os.path.join(MEMOIZE_DIR,
                        exec_hexdigest[0:2],
                        exec_hexdigest[2:4],
                        exec_hexdigest[4:])

def cache_makedirs(exec_hexdigest):
    path = cache_dir(exec_hexdigest)
    os.makedirs(name=path, exist_ok=True)
    return path


def store_file_list_pickle(base_path, file_list):
    """Store input file list FILE_LIST in a file at BASE_PATH."""

    with open(base_path + '.pickle', 'wb') as out_file:
        pickle.dump(file_list, out_file, pickle.DEFAULT_PROTOCOL)


def store_inputs(input_list_sorted_by_fname,
                 inputs_dir_path):

    inputs_path = os.path.join(inputs_dir_path, INPUTS_FILENAME)

    store_file_list_pickle(base_path=inputs_path,
                           file_list=input_list_sorted_by_fname)

    # human readable
    with open(inputs_path + '.txt', 'wb') as inputs_file:
        for input_fname, input_status in input_list_sorted_by_fname:
            if input_status[1] is None:
                hash_hex = b'_'
            else:
                hash_hex = binascii.hexlify(input_status[1])

            inputs_file.write(hash_hex +
                              SEPARATOR +
                              str(input_status[0]).encode('ascii') +
                              SEPARATOR +
                              input_fname +
                              b'\n')


def store_outputs(wr_list, inputs_dir_path):
    """Store output WR_LIST in directory INPUTS_DIR_PATH."""

    wr_list_path = os.path.join(inputs_dir_path, OUTPUTS_FILENAME)

    wr_stats_list = []
    for fname in sorted(wr_list):
        wr_stats_list.append((fname,
                              os.path.getmtime(fname),
                              hashlib.new(HASH_NAME, open(fname, 'rb').read()).digest()))

    store_file_list_pickle(base_path=wr_list_path,
                           file_list=wr_stats_list)

    # human readable
    with open(wr_list_path + '.txt', 'wb') as wr_list_file:
        for wr_fname in wr_list:

            # create artifact storage directory
            os.makedirs(name=ARTIFACT_DIR, exist_ok=True)

            # copy artifact to compressed storage
            hexdigest = hashlib.new(HASH_NAME,
                                    open(wr_fname, 'rb').read()).hexdigest()

            artifact_path = os.path.join(ARTIFACT_DIR, hexdigest + '.gz')
            if not os.path.exists(artifact_path):

                with open(wr_fname, 'rb') as wr_file:
                    with gzip.open(artifact_path, 'wb') as artifact_file:
                        shutil.copyfileobj(wr_file, artifact_file)

                # and store hexdigest modification time and path
                wr_list_file.write(hexdigest.encode('ascii') +
                                   SEPARATOR +
                                   str(os.path.getmtime(wr_fname)).encode('ascii') +
                                   SEPARATOR +
                                   wr_fname +
                                   b'\n')


def git_status(dname):
    """Return result of Git status of directory named DNAME as list relative name
and status string."""

    entries = []

    # TODO functionize to git_status(dir) which return a list
    if os.access(dname, os.W_OK): # if directory is writable
        # outputs may be written to `dname` so we check Git status for all files in that directory
        try:
            # http://stackoverflow.com/questions/42158080/git-status-in-machine-readable-format-with-paths-relative-to-current-directory

            args = ('git', 'status', '-s', '--ignored', dname.decode(ENCODING))
            git_status_separator = b'\n' # either b'\n' or b'\x00'

            output = subprocess.check_output(args=args,
                                             shell=False)
            for file_status in output.split(git_status_separator):
                if file_status:
                    entries.append(tuple(file_status.split(b' ')))
        except CalledProcessError:
            pass

    return entries

def dir_filenames_hash(dname, chash):
    """Hash dir contents of directory name DNAME as list of filenames into CHASH."""
    sub_fnames = sorted(os.listdir(dname))

    for status, sub_fname in git_status(dname):
        if status in [b'!!', b'??']:
            try:
                sub_fnames.remove(sub_fname)

                if status == b'!!':
                    print('warning: Skipping Git-ignored file {} in input directory "{}"'
                          .format(sub_fname, dname.decode(ENCODING)))
                elif status == b'??':
                    print('warning: Skipping Git-unknown file {} in input directory "{}"'
                          .format(sub_fname, dname.decode(ENCODING)))

            except ValueError:
                pass

    # hash number of files in dir named dname
    chash.update(str(len(sub_fnames)).encode(ENCODING))

    for sub_fname in sub_fnames:
        # hash dir sub file name
        chash_update_tree(chash, sub_fname)


def try_load_outputs_path_from_cache(cache_dir_path):
    try:
        cache_dir_inputs = os.listdir(cache_dir_path)
    except FileNotFoundError:
        return None

    for rw_base_filename in filter(lambda fname: fname.startswith(INPUTS_FILE_CONTENT_HASH_PREFIX),
                                   cache_dir_inputs):

        inputs_path = os.path.join(cache_dir_path, rw_base_filename, INPUTS_FILENAME) + '.pickle'
        with open(inputs_path, 'rb') as rw_pickle_file:
            rw_file_list = pickle.load(rw_pickle_file)
            dirty = False
            for rw_file in rw_file_list:
                input_fname = rw_file[0]
                input_mtime = rw_file[1][0]
                input_chash = rw_file[1][1]
                if file_content_is_modified_since(input_fname,
                                                  input_mtime,
                                                  input_chash):
                    if True:
                        print("warning: Cannot load from cache because input state of",
                              input_fname.decode(ENCODING),
                              "has changed from",
                              input_mtime, binascii.hexlify(input_chash))
                    dirty = True
                    break
            if not dirty:
                outputs_path = os.path.join(cache_dir_path, rw_base_filename, OUTPUTS_FILENAME) + '.pickle'
                return outputs_path

    return None


def update_cache(exec_state,
                 exec_chash,
                 fnames_all):

    cache_dir_path = cache_makedirs(exec_hexdigest=exec_chash.hexdigest())

    # fingerprint inputs
    inputs_chash = hashlib.new(HASH_NAME)

    # get inputs and outputs
    rw_list = {}       # read or/and written file names => mtime and optional chash
    wr_list = set()    # written file names
    for (syscall, fnames) in fnames_all.items():
        for fname in fnames:
            # skip temporary files by default
            if re.match(pattern=SKIP_FILE_RE,
                        string=bytes.decode(fname)):
                continue

            # skip non-existing (transient) files
            if not os.path.exists(fname):
                print("warning: Skipping non-existing file {}".format(fname))
                continue

            # optionally skip system inputs
            if SKIP_SYSTEM_INPUTS and re.match(pattern=SYSTEM_FILE_RE,
                                               string=bytes.decode(fname)):
                continue

            if syscall in ['stat', 'statfs']:
                if fname not in rw_list:
                    if os.path.isdir(fname):
                        pass    # do nothing for now as outputs may be written here
                    else:
                        if re.match(pattern=SYSTEM_FILE_RE,
                                    string=bytes.decode(fname)):
                            rw_list[fname] = (os.path.getmtime(fname),
                                              None)
            elif syscall == 'open_dir_rdonly':
                dir_hash = hashlib.new(HASH_NAME)
                dir_filenames_hash(fname, dir_hash)
                rw_list[fname] = (os.path.getmtime(fname),
                                  dir_hash.digest())
            elif syscall in ['open_rdonly',
                             'open_wronly',
                             'open_rdwr']:
                assert(not os.path.isdir(fname))

                rw_list[fname] = (os.path.getmtime(fname),
                                  hashlib.new(HASH_NAME, open(fname, 'rb').read()).digest())
                if syscall in ['open_wronly',
                               'open_rdwr']:
                    wr_list.add(fname)
            elif syscall in ['access']:
                pass
            else:
                raise Exception('TODO handle syscall ' + syscall)

    input_list = filter(lambda rw_entry: rw_entry[0] not in wr_list,
                        rw_list.items())
    input_list_sorted_by_fname = sorted(input_list) # TODO sort by first element only
    for rw_file, rw_status in input_list_sorted_by_fname:
        if os.path.isdir(rw_file):
            dir_filenames_hash(dname=rw_file,
                                chash=inputs_chash)
        else:
            if rw_status[1] != None:
                chash_update_tree(inputs_chash, rw_status[1]) # only content hash here

    inputs_dir_path = os.path.join(cache_dir_path,
                                   INPUTS_FILE_CONTENT_HASH_PREFIX +
                                   inputs_chash.hexdigest())

    # create input root directory
    os.makedirs(name=inputs_dir_path,  exist_ok=True)

    # write execve call
    conf_path = os.path.join(cache_dir_path,
                             'exec_state.conf')
    with open(conf_path,  'w') as conf_file:
        for key, value in exec_state:
            if key == 'env':
                conf_file.write('[' + key + ']\n')
                for env_entry in value:
                    conf_file.write(env_entry[0].decode(ENCODING) + '="' + env_entry[1].decode(ENCODING) + '"\n')
            else:
                if isinstance(value, str):
                    conf_file.write(key + '="' + value + '"\n')
                elif isinstance(value, bytes):
                    conf_file.write(key + '="' + value.decode(ENCODING) + '"\n')
                else:
                    conf_file.write(key + '="' + str(value) + '"\n')

    # open(os.path.join(cache_dir_path,
    #                   'exec_state.python-repr'),
    #      'w').write(repr(exec_state))

    store_inputs(input_list_sorted_by_fname=input_list_sorted_by_fname,
                 inputs_dir_path=inputs_dir_path)

    store_outputs(wr_list=wr_list,
                  inputs_dir_path=inputs_dir_path)

    return 'uncached'


def normalize_cwdrelpath(path):
    """Convert absolute paths under cwd to paths relative to cwd."""
    abs_cwd = os.path.abspath(os.getcwd()).encode('ascii') # TODO correct encoding?
    if os.path.isabs(path) and path.startswith(abs_cwd):
        return os.path.relpath(path, abs_cwd)
    else:
        return path


def process_strace_output_file(out_file):

    first_execve_state = None
    trace = []                  # trace tree

    # hash from system call name to call instances set
    fnames_all = {}
    fnames_all['open_rdonly']     = set()
    fnames_all['open_dir_rdonly'] = set()
    fnames_all['open_wronly']     = set()
    fnames_all['open_rdwr']       = set()
    fnames_all['stat']            = set()
    fnames_all['statfs']          = set()
    fnames_all['openat']          = set()
    fnames_all['access']          = set()

    out_file.seek(0)              # reset it
    for out_line in out_file:

        if b'ENOENT' in out_line: # skip failing calls
            continue

        # print(out_line)

        pid_, out_rest = out_line.split(None, 1)

        pid = int(pid_) # decode PID integer

        try:
            syscall, out_rest = out_rest.split(b'(', 1)
            syscall_s = bytes.decode(syscall)

            fname, out_rest = out_rest.split(b', ', 1)
            fname = os.path.normpath(fname[1:-1])

            if (fname.startswith(b'//usr') or
                fname.startswith(b'//lib')):
                fname = fname[1:] # skip first slash for now. TODO correct?

            fname = normalize_cwdrelpath(fname)

            # if syscall != b'execve':
            #     pp((syscall, fname, out_rest))

            # in decreasing probability
            if syscall == b'execve':
                # get args
                _args, out_rest = out_rest.split(b'], [', 1)
                argv = eval(_args[1:]) # tuple of strings

                # get environment
                _env, out_rest = out_rest.split(b']) = ', 1)
                env = eval(_env) # tuple of strings containing ENV_VAR=VALE

                # store and fingerprint
                exec_state = (syscall, fname, argv, env)
                if first_execve_state is None:
                    first_execve_state = exec_state
                    trace.append(exec_state)

            elif syscall == b'stat':
                fnames_all['stat'].add(fname)
            elif syscall == b'statfs':
                fnames_all['statfs'].add(fname)
            elif syscall == b'openat':
                fnames_all['openat'].add(fname)
            elif syscall == b'open':
                # TODO split out_rest at ending ')' and pattern match against all bitfields O_....
                if (out_rest.startswith(b'O_RDONLY|O_DIRECTORY') or
                    out_rest.startswith(b'O_RDONLY|O_NONBLOCK|O_DIRECTORY')): # read directory contents
                    fnames_all['open_dir_rdonly'].add(fname)
                elif out_rest.startswith(b'O_RDONLY'): # read-only
                    fnames_all['open_rdonly'].add(fname)
                elif out_rest.startswith(b'O_WRONLY'):
                    # write-only create into new file
                    fnames_all['open_wronly'].add(fname)
                elif out_rest.startswith(b'O_RDWR|O_CREAT|O_TRUNC'): # write-only
                    fnames_all['open_wronly'].add(fname)
                elif out_rest.startswith(b'O_RDWR|O_CREAT'): # read-write. TODO respect O_CREAT?
                    fnames_all['open_rdwr'].add(fname)
                elif out_rest.startswith(b'O_RDWR'): # read-write
                    fnames_all['open_rdwr'].add(fname)
                else:
                    raise Exception('unknown out_rest ' + str(out_rest))
            elif syscall == b'access':
                fnames_all['access'].add(fname)
            else:
                print('warning: Handle system call ' + syscall_s)

        except ValueError:
            pass

    return (first_execve_state, trace, fnames_all)


def which(program):
    """http://stackoverflow.com/questions/377017/test-if-executable-exists-in-python#377028"""

    def is_exe(fpath):
        return os.path.isfile(fpath) and os.access(fpath, os.X_OK)

    fpath, fname = os.path.split(program)
    if fpath:
        if is_exe(program):
            return program
    else:
        for path in os.environ["PATH"].split(os.pathsep):
            path = path.strip('"')
            exe_file = os.path.join(path, program)
            if is_exe(exe_file):
                return exe_file

    return None


def try_restore_outputs_from_cache(cached_outputs_path):
    """Try copying output files listed in CACHED_OUTPUTS_PATH from artifact cache."""

    cache_fetch_ok = True
    with open(cached_outputs_path, 'rb') as cached_outputs_pickle_file:
        cached_outputs_list = pickle.load(cached_outputs_pickle_file)
        for cached_output in cached_outputs_list:
            fname = cached_output[0]
            mtime = cached_output[1]
            chash = cached_output[2]

            if os.path.exists(fname):
                if file_content_is_modified_since(fname, mtime, chash):
                    cache_fetch_ok = False
                    break
                else:
                    pass    # output file already exists, nothing needed
            else:
                artifact_path = os.path.join(ARTIFACT_DIR,
                                             binascii.hexlify(chash).decode('ascii') + '.gz')
                try:
                    with gzip.open(artifact_path, 'rb') as artifact_file:
                        with open(fname, 'wb') as output_file:
                            if OPT_VERBOSE:
                                print('info: Fetched {} from cache'.format(fname.decode(ENCODING)))
                            shutil.copyfileobj(artifact_file, output_file)
                except:
                    print("warning: Failed to copy artifact {} to {}".format(artifact_path, fname))
                    cache_fetch_ok = False
                    break
    return cache_fetch_ok


def memoized_run(args,
                 trace_childs=True,
                 env=None):

    # state describe progress called with arguments (args)
    prog = which(args[0])
    sorted_env = sorted(os.environ._data.items()) # hashing needs deterministic order
    exec_state = (('prog', prog),
                  ('mtime', os.path.getmtime(prog)),
                  ('args', args),
                  ('cwd', os.getcwd()),
                  ('env', sorted_env))
    exec_chash = chash_exec_state(exec_state) # precalculation needed for speed

    # try fetch from cache
    cache_dir_path = cache_dir(exec_hexdigest=exec_chash.hexdigest())
    cached_outputs_path = try_load_outputs_path_from_cache(cache_dir_path=cache_dir_path)
    if cached_outputs_path:
        if try_restore_outputs_from_cache(cached_outputs_path=cached_outputs_path):
            sys.exit(0)         # TODO handle non-zero previous return code
        else:
            if OPT_VERBOSE:
                print('info: Cache fetch failed')

    indent = 0

    with tempfile.NamedTemporaryFile() as out_file:

        # collect flags
        strace_std_flags = ['-v',
                            '-s' + str(MAX_STRACE_STRING_LENGTH)]
        if trace_childs:
            strace_std_flags.append('-f')
        outfile_flags = ['-o', out_file.name]
        sys_calls_flags = ['-e', 'trace=' +
                           ','.join(SYS_CALLS + SYS_CALLS_AT)]
        strace_cmd = ['strace'] + strace_std_flags + outfile_flags + sys_calls_flags + args

        ret = subprocess.run(args=strace_cmd, shell=False)

        (_, trace, fnames_all) = process_strace_output_file(out_file=out_file)

        update_cache(exec_state=exec_state,
                     exec_chash=exec_chash,
                     fnames_all=fnames_all)

        if OPT_VERBOSE == 'full':

            print("\n============================= TRACE RESULTS ===================================\n")

            if OPT_VERBOSE:
                indent += 1
                _print_syscall_fnames(syscall_s='stat', fnames=fnames_all['stat'], indent=indent)
                _print_syscall_fnames(syscall_s='statfs', fnames=fnames_all['statfs'], indent=indent)
                _print_syscall_fnames(syscall_s='access', fnames=fnames_all['access'], indent=indent)
                _print_syscall_fnames(syscall_s='open-rdonly', fnames=fnames_all['open_rdonly'], indent=indent)
                _print_syscall_fnames(syscall_s='open-wronly', fnames=fnames_all['open_wronly'], indent=indent)
                _print_syscall_fnames(syscall_s='open-rdwr', fnames=fnames_all['open_rdwr'], indent=indent)
                _print_syscall_fnames(syscall_s='openat', fnames=fnames_all['openat'], indent=indent)

        return (ret, trace)


if __name__ == '__main__':

    opts, args = getopt.getopt(sys.argv[1:], 'dv:')

    for opt, value in opts:
        if opt == '-v':
            OPT_VERBOSE = True

    completedProcess, trace = memoized_run(args)
    sys.exit(completedProcess.returncode)
